{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, February 12, 2024\n",
    "\n",
    "* mamba install conda-forge::einops\n",
    "\n",
    "## Sunday, February 11, 2024\n",
    "\n",
    "Experiment with [HuggingFace Sentence Transformers](https://huggingface.co/sentence-transformers)\n",
    "\n",
    "## Tuesday, February 6, 2024\n",
    "\n",
    "OK Nice! Got this to run in the 'mls2' environment.\n",
    "\n",
    "## Monday, February 5, 2024\n",
    "\n",
    "A quick test to validate this environment is good to go with transformers.\n",
    "\n",
    "Hmm I have a local environment variable set for the HuggingFace Transformers model cache folder and yet, when I download a model here, it gets loaded into the default '~/cache/huggingface/hub' folder ... meh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "models--sentence-transformers--paraphrase-MiniLM-L6-v2\n",
      "tmp9s591511\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always start with making sure any cuda code will target the 4090."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's conduct a simple test using the [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default code shown in the Model card, the model gets loaded to the CPU Ram, then to the GPU VRAM where it runs out of GPU memory!\n",
    "\n",
    "Then when I try to load it directly to the GPU, it fails with the error:\n",
    "\n",
    "'ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`'\n",
    "\n",
    "So then I ran 'mamba install conda-forge::accelerate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4881771583724664b8266a3ba0a4583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This way of loading the model loads it to the CPU memory, NOT the GPU VRAM memory. \n",
    "# And when we try to then load it to the GPU, we run out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# mamba install conda-forge::accelerate\n",
    "\n",
    "# And when I tried this, after install accelerate, it still ran out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "\n",
    "\n",
    "# And when I run this, I get this error message:\n",
    "#   ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` \n",
    "#   and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or `pip install bitsandbytes`.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# mamba install conda-forge::bitsandbytes\n",
    "\n",
    "# Wow! Now when I run this, I get a ton of error messages related to CUDA ... like the following ...\n",
    "# CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# Running this generates the same mess of CUDA errors ... man, I got to wonder, do I need to install the CUDA Toolkit??\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                               load_in_8bit=True,\n",
    "#                                               device_map='auto',\n",
    "#                                               torch_dtype=torch.float16,\n",
    "#                                               low_cpu_mem_usage=True,\n",
    "#                                               )\n",
    "\n",
    "\n",
    "# So yeah, I actually just installed the CUDA 12.3 toolkit and we are still getting these CUDA errors! WTF!?\n",
    "\n",
    "\n",
    "# This code worked in another notebook but different model and within docker ...\n",
    "# I am now thinking this may have to do with 'bitsandbytes' problems ....\n",
    "# Yeah ... I think the solution to this is found in the error message itself ... I need to compile from source.\n",
    "# OK. This works now ...\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map=device,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              )\n",
    "\n",
    "# 13.0s\n",
    "# 8252 MiB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "\n",
    "# 48.0s\n",
    "# 12930 MiB VRAM\n",
    "\n",
    "# 37.3s\n",
    "# 12906 MiB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Of course, I'd be happy to help you make your own mayonnaise at home. Here's a simple and classic recipe that you can try:\n",
      "\n",
      "Ingredients:\n",
      "- 1 egg yolk\n",
      "- 1 tablespoon of Dijon mustard\n",
      "- 1 cup (200 ml) of vegetable oil (such as canola or sunflower oil)\n",
      "- 1-2 tablespoons of white wine vinegar or lemon juice\n",
      "- Salt to taste\n",
      "\n",
      "Instructions:\n",
      "1. Combine egg yolk and mustard in a medium-sized bowl.\n",
      "2. Whisk in 1 teaspoon of white wine vinegar or lemon juice.\n",
      "3. Gradually add oil in a thin, slow stream, whisking constantly to emulsify. If the mayonnaise thickens too much, add a small amount of water to thin it out.\n",
      "4. Once all the oil has been incorporated, whisk in remaining vinegar or lemon juice, and add salt to taste.\n",
      "\n",
      "This mayonnaise will keep in a tightly sealed container in the refrigerator for up to a week. Enjoy!</s>\n"
     ]
    }
   ],
   "source": [
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "models--sentence-transformers--paraphrase-MiniLM-L6-v2\n",
      "tmp9s591511\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "# sentence transformers reside in the same location as other transformers\n",
    "!ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# 1m 21.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nomic-ai/nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1)\n",
    "\n",
    "nomic-embed-text-v1 is 8192 context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "models--sentence-transformers--paraphrase-MiniLM-L6-v2\n",
      "tmp9s591511\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface/hub/models--nomic-ai--nomic-embed-text-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell for the first time generated the following error:\n",
    "\n",
    "ImportError: This modeling file requires the following packages that were not found in your environment: einops. Run `pip install einops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.3.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n",
    "\n",
    "# This message refers to the version of SentenceTransformers\n",
    "# You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.3.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0951355e-02  5.7414662e-02 -1.1036437e-02 ...  3.5154229e-05\n",
      "  -2.8092161e-02 -2.1599840e-02]\n",
      " [-1.3366990e-02  2.7091309e-02 -2.3367396e-02 ...  2.8799431e-02\n",
      "  -1.0674694e-02  2.8820794e-02]]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NomicBertModel(\n",
       "  (embeddings): NomicBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30528, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "  (emb_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (encoder): NomicBertEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x NomicBertBlock(\n",
       "        (attn): NomicBertAttention(\n",
       "          (rotary_emb): NomicBertDynamicNTKRotaryEmbedding()\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): NomciBertGatedMLP(\n",
       "          (fc11): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (fc12): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0951e-02,  5.7415e-02, -1.1036e-02,  ...,  3.5161e-05,\n",
      "         -2.8092e-02, -2.1600e-02],\n",
      "        [-1.3367e-02,  2.7091e-02, -2.3367e-02,  ...,  2.8799e-02,\n",
      "         -1.0675e-02,  2.8821e-02]])\n"
     ]
    }
   ],
   "source": [
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "print(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylocalseer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
