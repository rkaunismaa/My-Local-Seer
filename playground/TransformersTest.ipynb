{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, February 6, 2024\n",
    "\n",
    "OK Nice! Got this to run in the 'mls2' environment.\n",
    "\n",
    "## Monday, February 5, 2024\n",
    "\n",
    "A quick test to validate this environment is good to go with transformers.\n",
    "\n",
    "Hmm I have a local environment variable set for the HuggingFace Transformers model cache folder and yet, when I download a model here, it gets loaded into the default '~/cache/huggingface/hub' folder ... meh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "tmp9s591511\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always start with making sure any cuda code will target the 4090."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's conduct a simple test using the [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default code shown in the Model card, the model gets loaded to the CPU Ram, then to the GPU VRAM where it runs out of GPU memory!\n",
    "\n",
    "Then when I try to load it directly to the GPU, it fails with the error:\n",
    "\n",
    "'ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`'\n",
    "\n",
    "So then I ran 'mamba install conda-forge::accelerate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bbc7f6913542eaaf6cc97cdef7f342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This way of loading the model loads it to the CPU memory, NOT the GPU VRAM memory. \n",
    "# And when we try to then load it to the GPU, we run out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# mamba install conda-forge::accelerate\n",
    "\n",
    "# And when I tried this, after install accelerate, it still ran out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "\n",
    "\n",
    "# And when I run this, I get this error message:\n",
    "#   ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` \n",
    "#   and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or `pip install bitsandbytes`.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# mamba install conda-forge::bitsandbytes\n",
    "\n",
    "# Wow! Now when I run this, I get a ton of error messages related to CUDA ... like the following ...\n",
    "# CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# Running this generates the same mess of CUDA errors ... man, I got to wonder, do I need to install the CUDA Toolkit??\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                               load_in_8bit=True,\n",
    "#                                               device_map='auto',\n",
    "#                                               torch_dtype=torch.float16,\n",
    "#                                               low_cpu_mem_usage=True,\n",
    "#                                               )\n",
    "\n",
    "\n",
    "# So yeah, I actually just installed the CUDA 12.3 toolkit and we are still getting these CUDA errors! WTF!?\n",
    "\n",
    "\n",
    "# This code worked in another notebook but different model and within docker ...\n",
    "# I am now thinking this may have to do with 'bitsandbytes' problems ....\n",
    "# Yeah ... I think the solution to this is found in the error message itself ... I need to compile from source.\n",
    "# OK. This works now ...\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map=device,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              )\n",
    "\n",
    "# 13.0s\n",
    "# 8252 MiB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "\n",
    "# 48.0s\n",
    "# 12930 MiB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Yes, I do! Here's a simple homemade mayonnaise recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup vegetable oil (canola or safflower oil work best)\n",
      "- 1 egg yolk (at room temperature)\n",
      "- 1 tbsp white wine vinegar or lemon juice\n",
      "- 1 tbsp Dijon mustard\n",
      "- Salt and pepper, to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a medium-sized mixing bowl, whisk together the egg yolk, vinegar, and mustard until they are well combined.\n",
      "2. Slowly drizzle the oil into the egg yolk mixture, whisking constantly. Be sure to add the oil very slowly, dropping it in a thin, steady stream. This will help the mayonnaise emulsify.\n",
      "3. Once all of the oil has been incorporated, you can add any additional flavours you'd like, such as minced garlic, herbs, or spices.\n",
      "4. Season the mayonnaise with salt and pepper to taste.\n",
      "5. Cover the bowl with plastic wrap, making sure the wrap touches the surface of the mayonnaise to prevent a skin from forming.\n",
      "6. Refrigerate the mayonnaise for at least 1 hour before using it. It will keep in the refrigerator for up to a week.\n",
      "\n",
      "Enjoy your homemade mayonnaise!</s>\n"
     ]
    }
   ],
   "source": [
    "print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylocalseer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
