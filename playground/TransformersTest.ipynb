{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, February 6, 2024\n",
    "\n",
    "OK Nice! Got this to run in the 'mls2' environment.\n",
    "\n",
    "## Monday, February 5, 2024\n",
    "\n",
    "A quick test to validate this environment is good to go with transformers.\n",
    "\n",
    "Hmm I have a local environment variable set for the HuggingFace Transformers model cache folder and yet, when I download a model here, it gets loaded into the default '~/cache/huggingface/hub' folder ... meh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "tmp9s591511\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always start with making sure any cuda code will target the 4090."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's conduct a simple test using the [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default code shown in the Model card, the model gets loaded to the CPU Ram, then to the GPU VRAM where it runs out of GPU memory!\n",
    "\n",
    "Then when I try to load it directly to the GPU, it fails with the error:\n",
    "\n",
    "'ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`'\n",
    "\n",
    "So then I ran 'mamba install conda-forge::accelerate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1537eca40b8b4c44b4fc618b2676f4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This way of loading the model loads it to the CPU memory, NOT the GPU VRAM memory. \n",
    "# And when we try to then load it to the GPU, we run out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# mamba install conda-forge::accelerate\n",
    "\n",
    "# And when I tried this, after install accelerate, it still ran out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "\n",
    "\n",
    "# And when I run this, I get this error message:\n",
    "#   ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` \n",
    "#   and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or `pip install bitsandbytes`.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# mamba install conda-forge::bitsandbytes\n",
    "\n",
    "# Wow! Now when I run this, I get a ton of error messages related to CUDA ... like the following ...\n",
    "# CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# Running this generates the same mess of CUDA errors ... man, I got to wonder, do I need to install the CUDA Toolkit??\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                               load_in_8bit=True,\n",
    "#                                               device_map='auto',\n",
    "#                                               torch_dtype=torch.float16,\n",
    "#                                               low_cpu_mem_usage=True,\n",
    "#                                               )\n",
    "\n",
    "\n",
    "# So yeah, I actually just installed the CUDA 12.3 toolkit and we are still getting these CUDA errors! WTF!?\n",
    "\n",
    "\n",
    "# This code worked in another notebook but different model and within docker ...\n",
    "# I am now thinking this may have to do with 'bitsandbytes' problems ....\n",
    "# Yeah ... I think the solution to this is found in the error message itself ... I need to compile from source.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map=device,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Yes, I certainly do have a few mayonnaise recipes up my virtual sleeve. Here's a simple one for a classic mayo:\n",
      "\n",
      "1. In a bowl, whisk together 1 cup of vegetable oil, 1 egg yolk, 1 tablespoon of white wine vinegar, 1 tablespoon of Dijon mustard, and a pinch of salt until well combined.\n",
      "2. Whisk in 1-2 tablespoons of water, one teaspoon at a time, until desired consistency is reached. The mayonnaise should be thick and creamy.\n",
      "3. Taste and adjust seasonings, if necessary, adding more salt or vinegar as desired.\n",
      "\n",
      "You can also make a flavoured mayonnaise by whisking in herbs, spices, or other seasonings after the basic mayonnaise is made. Enjoy!\n",
      "\n",
      "Here's another version that uses an immersion blender for a smoother texture:\n",
      "\n",
      "1. In a tall, narrow container, combine 1 cup of vegetable oil, 1 egg yolk, 1 tablespoon of white wine vinegar, 1 tablespoon of Dijon mustard, and a pinch of salt.\n",
      "2. Immerse the immersion blender into the mixture, making sure the blade is touching the bottom of the container. Turn on the blender and slowly drizzle in the oil in a thin stream, aiming for the blade.\n",
      "3. Once all the oil is incorporated and the mayonnaise is thick and smooth, taste and adjust seasonings as desired. This method can also be used for flavoured mayonnaise.</s>\n"
     ]
    }
   ],
   "source": [
    "print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylocalseer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
