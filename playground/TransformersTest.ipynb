{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, February 6, 2024\n",
    "\n",
    "OK Nice! Got this to run in the 'mls2' environment.\n",
    "\n",
    "## Monday, February 5, 2024\n",
    "\n",
    "A quick test to validate this environment is good to go with transformers.\n",
    "\n",
    "Hmm I have a local environment variable set for the HuggingFace Transformers model cache folder and yet, when I download a model here, it gets loaded into the default '~/cache/huggingface/hub' folder ... meh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "tmp9s591511\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always start with making sure any cuda code will target the 4090."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's conduct a simple test using the [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default code shown in the Model card, the model gets loaded to the CPU Ram, then to the GPU VRAM where it runs out of GPU memory!\n",
    "\n",
    "Then when I try to load it directly to the GPU, it fails with the error:\n",
    "\n",
    "'ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`'\n",
    "\n",
    "So then I ran 'mamba install conda-forge::accelerate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f654368200284c44809c9e117a1c8927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This way of loading the model loads it to the CPU memory, NOT the GPU VRAM memory. \n",
    "# And when we try to then load it to the GPU, we run out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# mamba install conda-forge::accelerate\n",
    "\n",
    "# And when I tried this, after install accelerate, it still ran out of VRAM!\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "\n",
    "\n",
    "# And when I run this, I get this error message:\n",
    "#   ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` \n",
    "#   and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or `pip install bitsandbytes`.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# mamba install conda-forge::bitsandbytes\n",
    "\n",
    "# Wow! Now when I run this, I get a ton of error messages related to CUDA ... like the following ...\n",
    "# CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              device_map=device,\n",
    "#                                              load_in_8bit=True)\n",
    "\n",
    "# Running this generates the same mess of CUDA errors ... man, I got to wonder, do I need to install the CUDA Toolkit??\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                               load_in_8bit=True,\n",
    "#                                               device_map='auto',\n",
    "#                                               torch_dtype=torch.float16,\n",
    "#                                               low_cpu_mem_usage=True,\n",
    "#                                               )\n",
    "\n",
    "\n",
    "# So yeah, I actually just installed the CUDA 12.3 toolkit and we are still getting these CUDA errors! WTF!?\n",
    "\n",
    "\n",
    "# This code worked in another notebook but different model and within docker ...\n",
    "# I am now thinking this may have to do with 'bitsandbytes' problems ....\n",
    "# Yeah ... I think the solution to this is found in the error message itself ... I need to compile from source.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map=device,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Here's a simple and classic recipe for homemade mayonnaise:\n",
      "\n",
      "Ingredients:\n",
      "- 1 egg yolk\n",
      "- 1 tbsp Dijon mustard\n",
      "- 1 cup (200 ml) vegetable oil (such as canola or sunflower)\n",
      "- 2 tbsp white wine vinegar or lemon juice\n",
      "- Salt to taste\n",
      "\n",
      "Instructions:\n",
      "1. In a medium-sized bowl, whisk together the egg yolk and mustard.\n",
      "2. Slowly drizzle in the oil, drop by drop, constantly whisking the mixture to ensure it emulsifies (combines and thickens). You can start adding the oil in a thin, steady stream once you have incorporated the first few drops.\n",
      "3. When about a quarter of the oil has been incorporated, you can slowly increase the speed of the oil stream. Be very careful not to add the oil too quickly, as this may cause the mayonnaise to break or curdle.\n",
      "4. Once all of the oil has been incorporated, whisk in the vinegar or lemon juice.\n",
      "5. Season with salt to taste, and whisk until smooth.\n",
      "6. Cover the bowl with a clean kitchen towel or plastic wrap and refrigerate the mayonnaise for at least 1 hour before serving to allow the flavours to develop.\n",
      "\n",
      "Your homemade mayonnaise will keep for up to a week in the refrigerator. Enjoy!\n",
      "\n",
      "By the way, if you're worried about the risk of salmonella from using a raw egg yolk, you could use a commercial mayonnaise base, which is already pasteurized, or a cooked egg yolk instead â€“ but note that the texture and flavour might be slightly different.</s>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylocalseer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
